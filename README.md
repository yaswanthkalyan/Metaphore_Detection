# Metaphore Detection Using DistilBERT

## Overview
This repository contains the code and resources for a metaphor detection project using the DistilBERT algorithm. The goal of this project is to automatically identify and classify metaphors within a given text. DistilBERT, a distilled version of BERT (Bidirectional Encoder Representations from Transformers), is employed for its efficiency and effectiveness in natural language understanding tasks.

## Data
The data directory contains both raw and processed data used for training and evaluation. Ensure your data is formatted appropriately before running the preprocessing script.

## Model Training
The train.py script is responsible for training the metaphor detection model. You can customize hyperparameters and training settings in the script to fit your specific needs.

## Results
The results directory stores model performance metrics, visualizations, and any other relevant output. Check this directory for evaluation results and training logs.


